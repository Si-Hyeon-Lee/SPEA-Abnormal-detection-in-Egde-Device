{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "df = pd.read_csv(\"./spea_data_nvme/spea_pass_fail_data.csv\")\n",
    "X = df.iloc[:, :-1].values.astype(np.float32)\n",
    "y = df.iloc[:, -1].values.astype(np.int64)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN = [1.0330383038436266, 0.9921870873230328, -5.986730383311507e-05, 0.999709199079134, 0.00018871763767504268, 1.0001191663871114, 0.00010791785095278968, -6.889660576601636e-06, 0.9860516957259265, 0.9729566646191812, -3.6380720025332668e-06, 1.0248069517773497, 0.9910081398957061, 1.0001973061100071, -0.0001358640079276542, -0.00018716975807537948, -0.00023357994728190492, 1.050537061310052, 0.00041538498954872777, 0.00012647459659548423, 0.99868456479531, 0.9927704092292291, 0.9914302523915063, 0.9999569777123373, 1.0097155053653102, 0.9598040141656471, 1.1414477781874402e-05, 1.0344419401524945]\n",
      "STD  = [0.3652252280428611, 0.48739150905466955, 1.0076352657638126, 1.0493326559525138, 1.006322919804772, 1.4002176241388296, 1.0094259855862298, 1.0063325457549883, 0.505650927798952, 0.5253252364315946, 1.0060853076783716, 0.3809724692806453, 0.47504535280103805, 1.027789063737055, 1.0091309884792374, 1.0059060292420945, 1.0062963827082931, 0.16452628281899337, 1.0087259731914986, 1.008708139108654, 0.6002081989185843, 0.4999109708349692, 0.5654256631807738, 1.1936360604839695, 0.3974849745384516, 0.31342224983603495, 1.0063514133534008, 0.6753538036610093]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "print(\"MEAN =\", scaler.mean_.tolist())\n",
    "print(\"STD  =\", scaler.scale_.tolist()) # For normalize in Real System.(=Device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearStudent(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "students = {\n",
    "    \"linear\": LinearStudent(X_train.shape[1]),\n",
    "    \"sml_mlp\": SmallMLP(X_train.shape[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shlee/Desktop/spea_arnormal_detection/spea_arnormal_detection_env/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.load_model(\"./save/XGB/xgb.model\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=28):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(), # BCE Loss 였음.\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "mlp_teacher = MLP(in_dim=X_train.shape[1])\n",
    "mlp_teacher.load_state_dict(torch.load(\"./save/MLP/mlp.pt\", map_location=device))\n",
    "mlp_teacher.to(device).eval()\n",
    "\n",
    "tab_teacher = TabNetClassifier()\n",
    "tab_teacher.load_model(\"./save/TABNET/tabnet_best_model.zip\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    p_xgb = xgb.predict_proba(X_train)[:, 1]\n",
    "    p_tab = tab_teacher.predict_proba(X_train)[:, 1]\n",
    "    p_mlp = []\n",
    "    for i in range(0, len(X_train), 2048):\n",
    "        xb = torch.tensor(X_train[i:i+2048], device=device)\n",
    "        p_mlp.append(mlp_teacher(xb).cpu().numpy())\n",
    "    p_mlp = np.vstack(p_mlp).ravel()\n",
    "\n",
    "T = 4.0\n",
    "soft_logits = np.log(np.clip(np.stack([p_xgb, p_mlp, p_tab], 1), 1e-6, 1-1e-6))\n",
    "soft_logits /= T\n",
    "p_soft = torch.sigmoid(torch.tensor(soft_logits).mean(1)).numpy()\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(X_train),\n",
    "    torch.tensor(y_train),\n",
    "    torch.tensor(p_soft, dtype=torch.float32)\n",
    ")\n",
    "val_ds = TensorDataset(torch.tensor(X_val),\n",
    "                       torch.tensor(y_val, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear early-stop @ 26\n",
      "linear best ROC-AUC: 0.6833\n",
      "sml_mlp early-stop @ 35\n",
      "sml_mlp best ROC-AUC: 0.8227\n"
     ]
    }
   ],
   "source": [
    "a = 0.4\n",
    "epochs = 100\n",
    "patience_limit = 10\n",
    "\n",
    "criterion_hard = nn.BCELoss()\n",
    "criterion_soft = nn.BCELoss()\n",
    "\n",
    "for name, model in students.items():\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "    best_auc, patience = 0, 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb_hard, yb_soft in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb_hard = yb_hard.float().unsqueeze(1).to(device)\n",
    "            yb_soft = yb_soft.float().unsqueeze(1).to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = a*criterion_hard(out, yb_hard) + (1-a)*criterion_soft(out, yb_soft)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in val_loader:\n",
    "                preds.append(model(xb.to(device)).cpu().numpy())\n",
    "        preds = np.vstack(preds).ravel()\n",
    "        auc = roc_auc_score(y_val, preds)\n",
    "        if auc > best_auc:\n",
    "            best_auc, patience = auc, 0\n",
    "            torch.save(model.state_dict(), f\"./save/STUDENT/{name}.pt\")\n",
    "        else:\n",
    "            patience += 1\n",
    "        if patience >= patience_limit:\n",
    "            print(f\"{name} early-stop @ {epoch}\")\n",
    "            break\n",
    "    print(f\"{name} best ROC-AUC: {best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear: ACC 0.6137, AUC 0.6833\n",
      "sml_mlp: ACC 0.7356, AUC 0.8227\n"
     ]
    }
   ],
   "source": [
    "for name, cls in [(\"linear\", LinearStudent), (\"sml_mlp\", SmallMLP)]:\n",
    "    model = cls(X_train.shape[1]).to(device)\n",
    "    model.load_state_dict(torch.load(f\"./save/STUDENT/{name}.pt\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        for xb, _ in val_loader:\n",
    "            preds.append(model(xb.to(device)).cpu().numpy())\n",
    "    preds = np.vstack(preds).ravel()\n",
    "    acc = accuracy_score(y_val, preds>0.5)\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    print(f\"{name}: ACC {acc:.4f}, AUC {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pruned]  ACC 0.6675  AUC 0.7668\n",
      "[Quantized] ACC 0.6638  AUC 0.7620\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.nn.utils import prune\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def evaluate(model, dataloader, y_true):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in dataloader:\n",
    "            preds.append(model(xb).cpu().numpy())\n",
    "    preds = torch.vstack([torch.tensor(p) for p in preds]).numpy().ravel()\n",
    "    acc = accuracy_score(y_true, preds > 0.5)\n",
    "    auc = roc_auc_score(y_true, preds)\n",
    "    return acc, auc\n",
    "\n",
    "device = \"cpu\"\n",
    "in_dim  = X_val.shape[1]\n",
    "\n",
    "sml = SmallMLP(in_dim).to(device)\n",
    "sml.load_state_dict(torch.load(\"./save/STUDENT/sml_mlp.pt\", map_location=device))\n",
    "\n",
    "\n",
    "for module in sml.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.5)\n",
    "        prune.remove(module, \"weight\")\n",
    "\n",
    "pruned_acc, pruned_auc = evaluate(sml, val_loader, y_val)\n",
    "print(f\"[Pruned]  ACC {pruned_acc:.4f}  AUC {pruned_auc:.4f}\")\n",
    "\n",
    "torch.save(sml.state_dict(), \"./save/STUDENT/sml_mlp_pruned.pt\")\n",
    "\n",
    "quantized = torch.quantization.quantize_dynamic(\n",
    "    sml, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "q_acc, q_auc = evaluate(quantized, val_loader, y_val)\n",
    "print(f\"[Quantized] ACC {q_acc:.4f}  AUC {q_auc:.4f}\")\n",
    "\n",
    "torch.save(quantized.state_dict(), \"./save/STUDENT/sml_mlp_pruned_int8.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spea_arnormal_detection",
   "language": "python",
   "name": "spea_arnormal_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
